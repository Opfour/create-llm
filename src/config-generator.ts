import { Template } from './types/template';
import { ProjectConfig } from './prompts';

/**
 * ConfigGenerator creates llm.config.js files based on templates
 */
export class ConfigGenerator {
  /**
   * Generate llm.config.js content based on template and user config
   */
  generateConfig(projectConfig: ProjectConfig, template: Template): string {
    const { model, training, data, tokenizer } = template.config;

    const config = {
      model: {
        type: model.type,
        size: projectConfig.template,
        vocab_size: model.vocab_size,
        max_length: model.max_length,
        layers: model.layers,
        heads: model.heads,
        dim: model.dim,
        dropout: model.dropout
      },
      training: {
        batch_size: training.batch_size,
        learning_rate: training.learning_rate,
        warmup_steps: training.warmup_steps,
        max_steps: training.max_steps,
        eval_interval: training.eval_interval,
        save_interval: training.save_interval,
        optimizer: training.optimizer,
        weight_decay: training.weight_decay,
        gradient_clip: training.gradient_clip,
        mixed_precision: training.mixed_precision,
        gradient_accumulation_steps: training.gradient_accumulation_steps
      },
      data: {
        train_path: 'data/raw/train.txt',
        val_path: 'data/raw/val.txt',
        max_length: data.max_length,
        stride: data.stride,
        val_split: data.val_split,
        shuffle: data.shuffle
      },
      tokenizer: {
        type: projectConfig.tokenizer,
        vocab_size: tokenizer.vocab_size,
        min_frequency: tokenizer.min_frequency,
        special_tokens: tokenizer.special_tokens
      },
      checkpoints: {
        save_total_limit: 3,
        save_on_interrupt: true,
        resume_from_checkpoint: null
      },
      logging: {
        log_interval: 100,
        log_dir: 'logs',
        tensorboard: true,
        wandb: projectConfig.plugins.includes('wandb')
      },
      plugins: projectConfig.plugins,
      deployment: {
        huggingface: {
          repo_name: null,
          private: false
        },
        replicate: {
          model_name: null
        }
      }
    };

    return this.formatConfig(config, template);
  }

  /**
   * Format config object as JavaScript module with comments
   */
  private formatConfig(config: any, template: Template): string {
    const lines: string[] = [];

    lines.push('/**');
    lines.push(' * LLM Training Configuration');
    lines.push(` * Generated by create-llm for ${template.name} template`);
    lines.push(' * ');
    lines.push(' * This file controls all aspects of your LLM training.');
    lines.push(' * Edit values to customize your training setup.');
    lines.push(' */');
    lines.push('');
    lines.push('module.exports = {');
    lines.push('  // Model architecture configuration');
    lines.push('  model: {');
    lines.push(`    type: '${config.model.type}',              // Architecture type (gpt, bert, t5)`);
    lines.push(`    size: '${config.model.size}',              // Template size (tiny, small, base, custom)`);
    lines.push(`    vocab_size: ${config.model.vocab_size},        // Vocabulary size`);
    lines.push(`    max_length: ${config.model.max_length},          // Maximum sequence length`);
    lines.push(`    layers: ${config.model.layers},                // Number of transformer layers`);
    lines.push(`    heads: ${config.model.heads},                 // Number of attention heads`);
    lines.push(`    dim: ${config.model.dim},                 // Model dimension`);
    lines.push(`    dropout: ${config.model.dropout},             // Dropout rate`);
    lines.push('  },');
    lines.push('');
    lines.push('  // Training hyperparameters');
    lines.push('  training: {');
    lines.push(`    batch_size: ${config.training.batch_size},           // Training batch size`);
    lines.push(`    learning_rate: ${config.training.learning_rate},      // Learning rate`);
    lines.push(`    warmup_steps: ${config.training.warmup_steps},       // Warmup steps`);
    lines.push(`    max_steps: ${config.training.max_steps},        // Maximum training steps`);
    lines.push(`    eval_interval: ${config.training.eval_interval},      // Evaluation frequency (steps)`);
    lines.push(`    save_interval: ${config.training.save_interval},      // Checkpoint save frequency (steps)`);
    lines.push(`    optimizer: '${config.training.optimizer}',       // Optimizer type (adamw, adam, sgd)`);
    lines.push(`    weight_decay: ${config.training.weight_decay},       // Weight decay for regularization`);
    lines.push(`    gradient_clip: ${config.training.gradient_clip},        // Gradient clipping threshold`);
    lines.push(`    mixed_precision: ${config.training.mixed_precision},    // Use mixed precision (FP16) training`);
    lines.push(`    gradient_accumulation_steps: ${config.training.gradient_accumulation_steps}, // Gradient accumulation steps`);
    lines.push('  },');
    lines.push('');
    lines.push('  // Data configuration');
    lines.push('  data: {');
    lines.push(`    train_path: '${config.data.train_path}',  // Training data path`);
    lines.push(`    val_path: '${config.data.val_path}',      // Validation data path`);
    lines.push(`    max_length: ${config.data.max_length},          // Maximum sequence length`);
    lines.push(`    stride: ${config.data.stride},              // Sliding window stride for data processing`);
    lines.push(`    val_split: ${config.data.val_split},           // Validation split ratio (if no val_path)`);
    lines.push(`    shuffle: ${config.data.shuffle},            // Shuffle training data`);
    lines.push('  },');
    lines.push('');
    lines.push('  // Tokenizer configuration');
    lines.push('  tokenizer: {');
    lines.push(`    type: '${config.tokenizer.type}',              // Tokenizer type (bpe, wordpiece, unigram)`);
    lines.push(`    vocab_size: ${config.tokenizer.vocab_size},        // Vocabulary size`);
    lines.push(`    min_frequency: ${config.tokenizer.min_frequency},         // Minimum token frequency`);
    lines.push(`    special_tokens: ${JSON.stringify(config.tokenizer.special_tokens)}, // Special tokens`);
    lines.push('  },');
    lines.push('');
    lines.push('  // Checkpoint management');
    lines.push('  checkpoints: {');
    lines.push(`    save_total_limit: ${config.checkpoints.save_total_limit},      // Maximum checkpoints to keep (older ones deleted)`);
    lines.push(`    save_on_interrupt: ${config.checkpoints.save_on_interrupt},  // Save checkpoint on Ctrl+C`);
    lines.push(`    resume_from_checkpoint: ${config.checkpoints.resume_from_checkpoint}, // Path to checkpoint to resume from`);
    lines.push('  },');
    lines.push('');
    lines.push('  // Logging configuration');
    lines.push('  logging: {');
    lines.push(`    log_interval: ${config.logging.log_interval},        // Logging frequency (steps)`);
    lines.push(`    log_dir: '${config.logging.log_dir}',          // Log directory`);
    lines.push(`    tensorboard: ${config.logging.tensorboard},        // Enable TensorBoard logging`);
    lines.push(`    wandb: ${config.logging.wandb},             // Enable Weights & Biases logging`);
    lines.push('  },');
    lines.push('');
    lines.push('  // Plugin configuration');
    lines.push('  // Uncomment plugins you want to use');
    lines.push('  plugins: [');
    
    if (config.plugins.length > 0) {
      config.plugins.forEach((plugin: string) => {
        lines.push(`    '${plugin}',               // ${this.getPluginDescription(plugin)}`);
      });
    } else {
      lines.push('    // \'wandb\',               // Weights & Biases integration');
      lines.push('    // \'synthex\',             // SynthexAI synthetic data generation');
      lines.push('    // \'huggingface\',         // Hugging Face Hub integration');
    }
    
    lines.push('  ],');
    lines.push('');
    lines.push('  // Deployment configuration');
    lines.push('  deployment: {');
    lines.push('    huggingface: {');
    lines.push(`      repo_name: ${config.deployment.huggingface.repo_name},        // Hugging Face repo name (e.g., 'username/model-name')`);
    lines.push(`      private: ${config.deployment.huggingface.private},         // Make repo private`);
    lines.push('    },');
    lines.push('    replicate: {');
    lines.push(`      model_name: ${config.deployment.replicate.model_name},       // Replicate model name`);
    lines.push('    },');
    lines.push('  },');
    lines.push('};');
    lines.push('');

    return lines.join('\n');
  }

  /**
   * Get plugin description
   */
  private getPluginDescription(plugin: string): string {
    const descriptions: Record<string, string> = {
      wandb: 'Weights & Biases integration',
      synthex: 'SynthexAI synthetic data generation',
      huggingface: 'Hugging Face Hub integration'
    };
    return descriptions[plugin] || plugin;
  }

  /**
   * Generate config with additional comments and tips
   */
  generateConfigWithTips(projectConfig: ProjectConfig, template: Template): string {
    const baseConfig = this.generateConfig(projectConfig, template);
    const tips = this.generateConfigTips(template);
    
    return baseConfig + '\n' + tips;
  }

  /**
   * Generate helpful tips as comments at the end of config
   */
  private generateConfigTips(template: Template): string {
    const lines: string[] = [];
    
    lines.push('/**');
    lines.push(' * Configuration Tips:');
    lines.push(' * ');
    lines.push(` * Template: ${template.name.toUpperCase()}`);
    lines.push(` * Parameters: ${(template.config.model.parameters / 1_000_000).toFixed(0)}M`);
    lines.push(` * Hardware: ${template.config.hardware.recommended_gpu}`);
    lines.push(` * Training Time: ${template.config.hardware.estimated_training_time}`);
    lines.push(' * ');
    lines.push(' * Training Tips:');
    template.config.documentation.training_tips.forEach(tip => {
      lines.push(` * - ${tip}`);
    });
    lines.push(' * ');
    lines.push(' * Common Adjustments:');
    lines.push(' * - Reduce batch_size if running out of memory');
    lines.push(' * - Increase gradient_accumulation_steps to simulate larger batches');
    lines.push(' * - Adjust learning_rate if loss is unstable');
    lines.push(' * - Increase max_steps for better model quality');
    lines.push(' * - Enable mixed_precision to reduce memory usage');
    lines.push(' * ');
    lines.push(' * For more information, see the README.md file.');
    lines.push(' */');
    
    return lines.join('\n');
  }
}
